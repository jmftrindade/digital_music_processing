{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21M.387 Fundamentals of Music Processing\n",
    "## Chord Recognition 2\n",
    "\n",
    "In this lecture, we continue template-based chord detection, but using Hidden Markov Models to improve performance (also known as post-filtering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import IPython.display as ipd\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import fmplib as fmp\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "fmp.documentation_button()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Chapter 5.3 (pp 273 â€“ 293)\n",
    "\n",
    "<img src=\"images/book_cover.png\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Here we introduce another approach to Chord Recognition based on Hidden Markov Models. This is a more sophisticated analysis approach than just using chord templates. In this context it is sometimes known as a \"post filtering\" process.\n",
    "\n",
    "<img src=\"images/chord_recog_process.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chains\n",
    "\n",
    "### States and Probabilities\n",
    "\n",
    "<font color='red'>__Whiteboard__</font>: Markov States\n",
    "\n",
    "\n",
    "A model representing discrete __states__ and the __probabilities__ of transitioning between states.\n",
    "\n",
    "We define $I$ discrete states:\n",
    "\n",
    "$$\\mathcal{A} = \\lbrace \\alpha_1, \\alpha_2, \\dots, \\alpha_I \\rbrace $$\n",
    "\n",
    "The model moves from state to state at each time step. All together, we have a __sequence of states__\n",
    "\n",
    "$$ S = (s_1, s_2, s_3, \\dots, s_N) \\text{ where } s_n \\in \\mathcal{A}$$\n",
    "\n",
    "Next, we define the conditional probability of moving from one state to another state.\n",
    "\n",
    "$$ a_{ij} = P[s_{n+1} = \\alpha_j \\lvert s_n = \\alpha_i] $$\n",
    "\n",
    "These $a_{ij}$ coefficients are called __state transition probabilities__ and must adhere to:\n",
    "\n",
    "$$\\sum_{j=1}^I a_{ij} = 1$$\n",
    "\n",
    "Markov Chains have very bad memory. The current state depends only the previous state and no more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "We will use the states $\\mathcal{A}$ to represent chords. As an example:\n",
    "\n",
    "- $I = 3$\n",
    "- $\\mathcal{A} = \\lbrace \\mathbf{C, G, F} \\rbrace $\n",
    "- The $a_{ij}$s are defined below\n",
    "\n",
    "<img src=\"images/markov_chain.png\" width=500>\n",
    "\n",
    "Think of this table as a matrix: $\\text{From} \\times \\text{To}$\n",
    "\n",
    "To complete the model, we need a set of __initial state probabilities__\n",
    "\n",
    "$$ c_i = P[s_1 = \\alpha_i]$$\n",
    "\n",
    "And, of course:\n",
    "\n",
    "$$\\sum_{i=1}^I c_i = 1$$\n",
    "\n",
    "In addition to the example values above, let's set:\n",
    "\n",
    "$$ c_1 = 0.6, c_2 = 0.2, c_3 = 0.2$$\n",
    "\n",
    "These values can be captured by a matrix $A$ and a vector $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array( ((.8, .1, .1),(.2, .7, .1),(.1, .3, .6)) )\n",
    "C = np.array( (.6, .2, .2) )\n",
    "\n",
    "print('\\nA=\\n', A)\n",
    "print('\\nC=\\n', C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example State sequence: \n",
    "$$S_1 = (\\mathbf{C,C,C,G,G,F,F,C,C})$$\n",
    "\n",
    "It is easy to find the probability of this sequence occurring.\n",
    "\n",
    "<font color='red'>__Whiteboard__</font>: Sequence Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = np.array((0,0,0,1,1,2,2,0,0))\n",
    "P1 = fmp.markov_sequence_prob(S1, C, A)\n",
    "\n",
    "print(\"Sequence:\", S1)\n",
    "print(\"Probability of Sequence:\", P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P[S_1]$ is very small. But this point is to _compare_ the probabilities of _different_ sequences.\n",
    "\n",
    "For example:\n",
    "$$S_2 = (\\mathbf{F,C,F,C,F,C,F,C,F})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2 = np.array((2,0,2,0,2,0,2,0,2))\n",
    "P2 = fmp.markov_sequence_prob(S2, C, A)\n",
    "\n",
    "print(\"Sequence:\", S2)\n",
    "print(\"Probability of Sequence: %.10f\" % P2)\n",
    "\n",
    "print(\"P[S1] / P[S2]:\", P1 / P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of $S_1$ happening is about 65,000 times the likelihood of $S_2$ happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Generator\n",
    "\n",
    "It is also easy enough to generate a sequence given a Markov Model:\n",
    "- use a random number generator to generate the initial state.\n",
    "- use a random number generator to generate the next state given the current state\n",
    "- repeat\n",
    "\n",
    "<font color='red'>__Whiteboard__</font>: Sequence Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = fmp.markov_generate(C, A, 17)\n",
    "print('states:', S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = [([60, 67, 65][n]+c, i*.35, .3) for i,n in enumerate(S) for c in (0,4,7)]\n",
    "chords_snd = fmp.synthesize_sequence(notes, 22050)\n",
    "ipd.Audio(chords_snd, rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models\n",
    "\n",
    "<font color='red'>__Whiteboard__</font>: HMMs and observation layer\n",
    "\n",
    "An additional property is added to the model: the _observation layer_.  \n",
    "\n",
    "- When a state is active, it produces an observation.\n",
    "- There are a total of $K$ discrete observations possible:\n",
    "$$\\mathcal{B} = \\lbrace \\beta_1, \\beta_2, \\dots, \\beta_K \\rbrace$$\n",
    "- There is a set of probabilities $b_{ik}$ for state $\\alpha_i$ to produce an observation $\\beta_k$. And:\n",
    "\n",
    "$$\\sum_{k=1}^K b_{ik} = 1$$\n",
    "\n",
    "- The underlying Markov model is hidden: we can't directly observe the states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "In our case:\n",
    "- The states are chords\n",
    "- The observations are chroma vectors.\n",
    "- We can observe the chroma vectors, but we cannot directly observe the underlying chord.\n",
    "\n",
    "<img src=\"images/emission_probs.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = fmp.harmonics_chord_template([0,4,7], 0.7)  # C template\n",
    "b1 = fmp.harmonics_chord_template([2,7,11], 0.7) # G template\n",
    "b2 = fmp.harmonics_chord_template([0,5,9], 0.7)  # F template\n",
    "B_set = np.array((b0, b1, b2))\n",
    "plt.imshow(B_set.T, origin='lower', cmap='Greys')\n",
    "plt.title(\"observations\")\n",
    "plt.show();\n",
    "\n",
    "B = np.array(( (0.7, 0, 0.3), (.1, .9, 0), (0, 0.2, .8)))\n",
    "print('emission probs\\n', B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete States\n",
    "\n",
    "In general, the actual observation set not may be a finite set. For our chords, chroma vectors are features in $\\mathbb{R}^{12}$ - an infinite set. Some fancier HMMs are _continuous HMMs_ where $B$ is described by _probability density functions_. We use _discrete HMMs_ as follows:\n",
    "\n",
    "- Create a _codebook_ of finite observations.\n",
    "- This can be done by hand or can be learned by using a clustering algorithm.\n",
    "- Create a mapping process between actual observations and the codebook set (a.k.a _quantization_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formal Definition of the Model\n",
    "\n",
    "- $I$ discrete states:\n",
    "$$\\mathcal{A} = \\lbrace \\alpha_1, \\alpha_2, \\dots, \\alpha_i \\rbrace $$\n",
    "- $A$ = ($I \\times I$) matrix of transition probabilities\n",
    "- $C$ = length $I$ vector of initial probabilities\n",
    "- $K$ discrete observations:\n",
    "$$\\mathcal{B} = \\lbrace \\beta_1, \\beta_2, \\dots, \\beta_k \\rbrace$$\n",
    "- $B$ = ($I \\times K$) matrix of emission probabilities\n",
    "\n",
    "These 5 parameters define the entire HMM:  \n",
    "$$\\Theta = (\\mathcal{A}, A, C, \\mathcal{B}, B)$$\n",
    "\n",
    "- $\\mathcal{A}$ and $\\mathcal{B}$ are fixed.\n",
    "- $A, C, B$ are free parameters that need to be determined somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM as a generator\n",
    "\n",
    "Given the HMM mode $\\Theta = (\\mathcal{A}, A, C, \\mathcal{B}, B)$, it is easy enough to generate an observation sequence:\n",
    "\n",
    "$$O = (o_1, o_2, \\dots , o_N)$$\n",
    "\n",
    "For example, let's create a sequence of 20 observations based on the example model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S, O = fmp.hmm_generate(C, A, B, 20)\n",
    "print('states:', S)\n",
    "print('observ:', O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Problem Formulations\n",
    "\n",
    "Our problem is the reverse. We are given $O$, and need to find the underlying hidden states that \"best explain\" $O$ given model parameters $\\Theta$.\n",
    "\n",
    "There are three problems generally solvable with the HMM structure:\n",
    "- Evaluation Problem: Given $O$ and $\\Theta$, what is the probably of $O$?\n",
    "- Estimation Problem: Given $O$, what are the model parameters $A,C,B$?\n",
    "- Uncovering Problem: Given $O$ and $\\Theta$, what state sequence $S$ best explains $O$? \n",
    "\n",
    "We are interested in the last problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Viterbi Algorithm\n",
    "\n",
    "Given $O$ with $N$ observations:  \n",
    "The problem is to find the \"best\" or \"highest-scoring\" or \"most likely\" sequence $S^*$ from all possible sequences of length $N$.\n",
    "\n",
    "The Viterbi algorithm is a dynamic programming algorithm applied to HMMs. See page 284 in the text book for a deeper explanation\n",
    "\n",
    "<img src=\"images/viterbi.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Example\n",
    "\n",
    "<img src=\"images/viterbi_toy.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to Chord Recognition\n",
    "\n",
    "### Hidden states\n",
    "States $\\mathcal{A} = \\lbrace \\text{12 major and 12 minor chords} \\rbrace$. $I=24$\n",
    "\n",
    "### Observations Codebook\n",
    "\n",
    "- We define a codebook of $K=24$ output states.\n",
    "- Instead of using a learning algorithm, simply use the 24 template vectors with a suitable value for $\\alpha$, the decay parameter.\n",
    "\n",
    "$\\mathcal{B}$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triads = fmp.make_triads_templates(0.7)\n",
    "plt.imshow(triads.T, origin='lower', cmap='Greys')\n",
    "fmp.chord_template_labels(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To map a real observation to the codebook, find the closest match using the dot product.\n",
    "\n",
    "This is exactly the same procedure as the template matching algorithm from before.\n",
    "\n",
    "Example, an observation from _Let It Be_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case with beatles\n",
    "snd1 = fmp.load_wav(\"audio/beatles_let_it_be.wav\",0,30)\n",
    "fs = 22050.\n",
    "ipd.Audio(snd1[0:int(fs*5)], rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_len = 4096\n",
    "hop_size = fft_len // 2\n",
    "ff = fs / hop_size\n",
    "chroma = fmp.make_chromagram(snd1, fs, fft_len, hop_size, gamma=1.0)\n",
    "plt.imshow(chroma[:,0:100], origin='lower', aspect='auto', cmap=\"Greys\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick an example chroma vector at 2 seconds ($n=22$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_obs = chroma[:,22]\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.atleast_2d(ex_obs).T, origin='lower', cmap='Greys')\n",
    "plt.title(\"Sample Observation\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(triads.T, origin='lower', cmap='Greys')\n",
    "plt.title(\"Observations Codebook\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.dot(triads, ex_obs)\n",
    "match = np.argmax(scores)\n",
    "print('best match:', match)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Actual Observation at $n=22$\")\n",
    "plt.imshow(np.atleast_2d(ex_obs).T, origin='lower', cmap='Greys')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"$o[22]$\")\n",
    "plt.imshow(np.atleast_2d(triads[match]).T, origin='lower', cmap='Greys')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Probabilities\n",
    "\n",
    "How to create matrix $A$?\n",
    "\n",
    "In music, some transitions are more likely than others.\n",
    "- $C \\rightarrow G$ is more likely than $C \\rightarrow F \\sharp$\n",
    "- $C \\rightarrow C$ (_self-transition_ probabilities) should always be high.\n",
    "\n",
    "Options:\n",
    "- Can create $A$ by hand using expert musical knowledge and knowledge of style / genre.\n",
    "- Can learn $A$ by using labeled observations\n",
    "\n",
    "We will use the simplest possible hand-constructed matrix for $A$:\n",
    "- high values along the diagonal (self-transition)\n",
    "- low values everywhere else\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the transition probabilities:\n",
    "trans_probs = fmp.make_regular_transition_probs(10.)\n",
    "\n",
    "plt.imshow(trans_probs, origin='lower', cmap=\"Greys\", vmin=0, vmax=1);\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Probabilities\n",
    "\n",
    "Here, don't assume anything a priori. So, $C$ is simply a uniform probability vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_probs = np.ones(24) /24 \n",
    "print(init_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission Probabilities\n",
    "\n",
    "Finally, we need Matrix $B$:\n",
    "- For a given chord (state), what is the probability distribution amongst the different possible observations?\n",
    "- It is not just 1-1. Confusion can easily happen.\n",
    "- A particular chord ($\\mathbf{C}$) may produce an observation that looks like a $\\mathbf{C}$, $\\mathbf{Am}$, or $\\mathbf{Em}$ template. But it is much more likely to produce $\\mathbf{C}$ or $\\mathbf{Am}$ than, say $\\mathbf{F \\sharp}$.\n",
    "\n",
    "To create $B$, assign probability values by seeing how close one template is to another. Templates that are closer produce a higher probability of observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emis_probs = fmp.make_emission_probs(0.7)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(emis_probs, origin='lower',cmap=\"Greys\")\n",
    "fmp.chord_template_labels(0)\n",
    "fmp.chord_template_labels(1)\n",
    "plt.colorbar();\n",
    "plt.ylabel('States', fontsize=15)\n",
    "plt.xlabel('Emission Probabilities', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncovering the Hidden States\n",
    "\n",
    "Our model $\\Theta$ is now fully defined.\n",
    "\n",
    "We'll use the python module `hmmlearn` to run the Viterbi algorithm on an observation sequence given our model parameters.\n",
    "\n",
    "The discrete HMM in hmmlearn is called `MultinomialHMM`.  \n",
    "Finding the optimal hidden state sequence is done with the `decode()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn import hmm\n",
    "\n",
    "# initialize the HMM with the number of states\n",
    "model = hmm.MultinomialHMM(n_components=24)\n",
    "\n",
    "# set the probabilities\n",
    "model.transmat_ = trans_probs\n",
    "model.startprob_ = init_probs\n",
    "model.emissionprob_ = emis_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an observation sequence\n",
    "observations = np.array((0,0,0,1,0,0,9,7,7,7,7,7,7,7))\n",
    "obs_column = observations.reshape(-1, 1) # must be converted to a column vector\n",
    "\n",
    "# find the optimal state sequence\n",
    "est = model.decode(obs_column)[1]\n",
    "\n",
    "print('observations:', observations)\n",
    "print('state est   :', est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song Examples\n",
    "\n",
    "The Beatles _Let It Be_\n",
    "\n",
    "Create chromagram features with:\n",
    "- FFT size: $N=4096$\n",
    "- Hop size: $H=2048$\n",
    "- Logarithmic compression factor $\\gamma = 1.0$\n",
    "- Euclidean normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case with Beatles\n",
    "\n",
    "fs = 22050.\n",
    "fft_len = 4096\n",
    "hop_size = fft_len // 2\n",
    "ff = fs / hop_size\n",
    "\n",
    "chroma = fmp.make_chromagram(snd1, fs, fft_len, hop_size, gamma=1.)\n",
    "plt.imshow(chroma, origin='lower', aspect='auto', cmap=\"Greys\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map observations (chroma vectors) to the codebook of 24 observations / templates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.dot(triads, chroma)\n",
    "observations = np.argmax(probs, axis=0)\n",
    "\n",
    "fmp.plot_matrix_and_points(probs, observations)\n",
    "fmp.chord_template_labels(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is exactly the same as our chord estimating process from last time.\n",
    "\n",
    "But now, we consider these to be _observations_ of a HMM. So we use them to uncover the hidden states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_column = observations.reshape(-1, 1)\n",
    "hmm_estimates = model.decode(obs_column)[1]\n",
    "\n",
    "fmp.plot_matrix_and_points(probs, hmm_estimates)\n",
    "fmp.chord_template_labels(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing against Ground Truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref1 = fmp.load_chord_annotations(\"audio/beatles_let_it_be.lab\", ff)\n",
    "\n",
    "print(f\"template only: {fmp.calc_chord_score(observations, ref1):.3f}\")\n",
    "print(f\"HMM estimats:  {fmp.calc_chord_score(hmm_estimates, ref1):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Reference annotations')\n",
    "fmp.plot_matrix_and_points(probs, ref1)\n",
    "fmp.chord_template_labels(0)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Observations / template-based maxima')\n",
    "fmp.plot_matrix_and_points(probs, observations)\n",
    "fmp.chord_template_labels(0)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('HMM estimates')\n",
    "fmp.plot_matrix_and_points(probs, hmm_estimates)\n",
    "fmp.chord_template_labels(0)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Time-smoothed (pre-filtering) template-based maxima')\n",
    "chroma_filt = fmp.temporal_smoothing(chroma, 12)\n",
    "templates_filt = np.dot(triads, chroma_filt)\n",
    "estimates_filt = np.argmax(templates_filt, axis=0)\n",
    "fmp.plot_matrix_and_points(templates_filt, estimates_filt)\n",
    "fmp.chord_template_labels(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_estimation(chroma, alpha, diag_ratio):\n",
    "    # emission probabilities\n",
    "    emis_probs = fmp.make_emission_probs(alpha)\n",
    "\n",
    "    # transition probabilities:\n",
    "    trans_probs = fmp.make_regular_transition_probs(diag_ratio)\n",
    "    \n",
    "    # initial probabilities:\n",
    "    init_probs = np.ones(24) / 24.\n",
    "\n",
    "    # create observations:\n",
    "    observations = fmp.estimate_chords(chroma, alpha)\n",
    "    obs_column = observations.reshape(-1, 1)\n",
    "\n",
    "    # create HMM model\n",
    "    model = hmm.MultinomialHMM(n_components=24)\n",
    "    model.transmat_ = trans_probs\n",
    "    model.startprob_ = init_probs\n",
    "    model.emissionprob_ = emis_probs\n",
    "    \n",
    "    # run HMM model:\n",
    "    return model.decode(obs_column)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_per_diag_r(snd, ref):\n",
    "    chroma = fmp.make_chromagram(snd, fs, fft_len, hop_size, gamma=1)\n",
    "    diag_r = np.arange(1, 100, 2)\n",
    "    score = np.zeros(len(diag_r))\n",
    "    for i, r in enumerate(diag_r):\n",
    "        hmm_est = hmm_estimation(chroma, 0.7, r)\n",
    "        score[i] = fmp.calc_chord_score(hmm_est, ref)\n",
    "    return diag_r, score\n",
    "\n",
    "diag_r, score = score_per_diag_r(snd1, ref1)\n",
    "plt.figure()\n",
    "plt.plot(diag_r, score)\n",
    "plt.title('Let It Be: HMM estimate score per diagonal r')\n",
    "plt.xlabel(\"Diagonal Ratio\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "state": {
    "c2ae07ca94024ae7aa83e741a34e6919": {
     "views": [
      {
       "cell_index": 51
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
