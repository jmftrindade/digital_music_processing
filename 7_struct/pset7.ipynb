{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21M.387 Fundamentals of Music Processing\n",
    "## Problem Set 7: Structure Analysis\n",
    "\n",
    "Make sure all your answers and plots display when the code block is run. You can leave word-based answers in code comments or markdown cells.\n",
    "\n",
    "You may use any fmplib functions from __previous__ units in your answers. You may __not__ use any fmplib functions from the current unit in your answers (unless explicitly noted). But you can use the current unit's fmplib for testing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from ipywidgets import interact\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import fmplib as fmp\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "fmp.documentation_button()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "The following are two sequences of song boundaries:\n",
    "\n",
    "- `est` is an algorithmically generated estimate of song boundaries and is given as sample locations at a feature rate of $F_f = 10\\text{Hz}$.\n",
    "- `ref` is the ground truth / reference annotation, given in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = np.load('data/ex1_est.npy')\n",
    "ref = np.load('data/ex1_ref.npy')\n",
    "ff = 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Write the function `comparison_plot(bounds1, bounds2)` that creates a plot of two sequences _on the same plot_. Find a good way of displaying this information so that both sequences are shown and are easy to visually compare (so you can eyeball which markers match and which do not). \n",
    "\n",
    "You can use `plt.vlines()`, `plt.plot()` with `ro`, or any other plotting function that works for you. Using colors can also be useful.\n",
    "\n",
    "Of course, for the comparison to make sense, the inputs to comparison_plot must be in the same time scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparions_plot(bounds1, bounds2):\n",
    "    pass\n",
    "\n",
    "# call your function here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Write the function `compare_boundaries(est, ref, tau)` with inputs:\n",
    "\n",
    "- `est`: an `np.array` of estimated boundary locations\n",
    "- `ref`: an `np.array` of the ground truth boundary locations\n",
    "- `tau`: the distance metric used to determine a match\n",
    "\n",
    "Return a tuple of three numbers:\n",
    "\n",
    "- the number of True Positives\n",
    "- the number of False Positives\n",
    "- the number of False Negatives\n",
    "\n",
    "When comparing an estimated point $b^{\\text{Est}}$ and reference point $b^{\\text{Ref}}$, consider\n",
    "$$\\vert b^{\\text{Est}} - b^{\\text{Ref}} \\vert < \\tau$$\n",
    "as an indication of a match.\n",
    "\n",
    "Caution: careful not to double count matches in cases where two estimates $b^{\\text{Est}}_i$ and $b^{\\text{Est}}_j$ are both close to the same $b^{\\text{Ref}}$. In this case, one estimate should count as a True Positive, while the other as a False Positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_boundaries(est, ref, tau):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the True Positive, False Positive, and False Negative counts for the Exercise 1 estimate and reference sequences using $\\tau = 1$ and $\\tau = 3$ seconds?\n",
    "\n",
    "Store your answers as tuples in the variables below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2_t1 = ...\n",
    "ex2_t3 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Write a function that returns the Precision, Recall, and F-Measure of two boundary sequences. This function can make use of the `compare_boundaries()` above.\n",
    "\n",
    "Inputs: same as in Exercise 2.\n",
    "\n",
    "Outputs:\n",
    "A tuple of the calculated values for Precision, Recall, and F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_boundaries_PRF(est, ref, tau):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the values of Precision, Recall, and F-measure for the above sequences using $\\tau = 1$ and $\\tau = 3$ seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex3_t1 = ...\n",
    "ex3_t3 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4a\n",
    "\n",
    "In the next three Exercises, you will create the algorithm for section boundary detection using Structure Features.\n",
    "\n",
    "- Load the audio below.\n",
    "- Create $\\mathbf{S}$, the self-similarity matrix using `audio_to_ssm()` with CENS parameters $l = 25$ and $d = 3$.\n",
    "- Plot $\\mathbf{S}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snd = fmp.load_wav('audio/brahms_hungarian_dance_5a.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_ssm(snd, cens_filt_len, cens_ds):\n",
    "    'Returns the SSM and its feature rate'\n",
    "    fft_len = 4096\n",
    "    hop_size = fft_len // 2\n",
    "    chroma = fmp.make_chromagram(snd, 22050., fft_len, hop_size)\n",
    "    chroma = fmp.cens(chroma, cens_filt_len, cens_ds)\n",
    "    ff = 22050. / hop_size / cens_ds\n",
    "    return np.dot(chroma.T, chroma), ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and plot the SSM:\n",
    "ssm, ff = ...,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create $\\mathbf{S}_D$ by applying diagonal smoothing to $\\mathbf{S}$ to enhance the diagonal lines of the matrix.\n",
    "\n",
    "- Use `fmp.diagonal_smoothing(m, win_len)` with a window length of 5 seconds. Note that the function expects `win_len` ($L_D$) to be in units of samples, not seconds.\n",
    "- Plot $\\mathbf{S}_D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and plot S_D\n",
    "ssm_d = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4b\n",
    "\n",
    "Following the text's explanation of thresholding, starting on page 192, write the function `threshold_and_scale()` as follows:\n",
    "\n",
    "Inputs:\n",
    "- `ssm`: $\\mathbf{S}$, a self-similarity matrix\n",
    "- `tau`: $\\tau$, the threshold value\n",
    "\n",
    "Output:\n",
    "- The resulting SSM\n",
    "\n",
    "Notes:\n",
    "- Assume $\\mathbf{S}[n,m] \\in [0, 1]$.\n",
    "- Apply a threshold. For every value of $\\mathbf{S}[n,m]$, set $\\mathbf{S}_{\\tau}[n,m] = 0$ if $\\mathbf{S}[n,m] < \\tau$. \n",
    "- Apply a scaling such that values in the range $[\\tau, 1]$ of $\\mathbf{S}_{\\tau}$ are linearly scaled to the range $[0, 1]$ and all other values $(< \\tau)$, are set to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_and_scale(ssm, tau):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your function:\n",
    "- Create $\\mathbf{S}_{\\tau}$ from $\\mathbf{S}_D$ with $\\tau = 0.85$.\n",
    "- Plot $\\mathbf{S}_{\\tau}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_t = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Convert the enhanced SSM into a structure features matrix as follows:\n",
    "\n",
    "Write the function `create_lag_matrix()` which creates a circular lag matrix\n",
    "\n",
    "Inputs:\n",
    "- `ssm`: $\\mathbf{S}$, a self-similarity matrix\n",
    "\n",
    "Output:\n",
    "- The resulting circular lag matrix, $\\mathbf{L^{\\circ}}$\n",
    "\n",
    "As described on page 213 of the text:\n",
    "$$\\mathbf{L^{\\circ}}[l,n] = \\mathbf{S}[(n+l) \\text{ mod } N, n]$$\n",
    "\n",
    "While this notation may be a bit confusing, the diagrams help clarify this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_matrix(ssm):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test,\n",
    "- Create $\\mathbf{L}^{\\circ}$ from $\\mathbf{S}_{\\tau}$\n",
    "- Plot $\\mathbf{L}^{\\circ}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6a\n",
    "\n",
    "The final step is to create a boundaries novelty curve by applying a derivative function to $\\mathbf{L}^{\\circ}$.\n",
    "\n",
    "- Create an $M \\times L_K$ derivative kernel $\\mathbf{K}_{\\Delta}$ using `fmp.make_derivative_kernel(rows, k_len)`. Set $M$ to be the number of rows of $\\mathbf{S}$ and $L_K$ to be the equivalent of 20 seconds. As before $L_K$ is in units of samples, not seconds.\n",
    "- Plot $\\mathbf{K}_{\\Delta}$\n",
    "- Plot a single row of $\\mathbf{K}_{\\Delta}$ to better see the profile of this matrix. Any row is fine since all rows are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6b\n",
    "\n",
    "As outlined in the lecture notes, use $\\mathbf{K}_{\\Delta}$ to create $\\Delta_S[n]$:\n",
    "\n",
    "- Slide $\\mathbf{K}_{\\Delta}$ horizontally along $\\mathbf{L^{\\circ}}$. At each time step $n$:\n",
    "    - Compute $\\mathbf{D}^{[n]} = \\mathbf{K}_{\\Delta}^{[n]} \\odot \\mathbf{L^{\\circ}}$, where $\\mathbf{K}_{\\Delta}^{[n]}$ is the kernel centered at $n$, and $\\odot$ means element-wise multiplication.\n",
    "    - Sum the rows to produce a column vector $l_{\\Delta}^{[n]} = \\sum_j \\mathbf{D}^{[n]}[m,j]$ \n",
    "    - The magnitude ($L_2$ norm) of $l_{\\Delta}^{[n]}$ represents the likelihood of a structural boundary at $n$.\n",
    "\n",
    "- $\\Delta_S[n] = \\Vert l_{\\Delta}^{[n]} \\Vert$ is the __structure novelty function__.\n",
    "\n",
    "Write the function `column_derivative(lag_matrix, k_len)` which produces $\\Delta_S[n]$.\n",
    "\n",
    "Inputs:\n",
    "\n",
    "- `lag_matrix`: $\\mathbf{L^{\\circ}}$\n",
    "- `k_len`: $L_K$, the length of the derivative kernel.\n",
    "\n",
    "Outputs:\n",
    "\n",
    "- $\\Delta_S[n]$, which should be the same length as the number of columns in the input matrix.\n",
    "\n",
    "You should use `fmp.make_derivative_kernel()` that you tested in Exercise 6a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_derivative(lag_matrix, k_len):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, test your function using $\\mathbf{L^{\\circ}}$ and $L_K$ from above (setting `nov_s`) and plot the resulting curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nov_s = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6c\n",
    "\n",
    "The boundary location estimates are the peaks of this novelty curve. \n",
    "\n",
    "Use `fmp.find_region_peaks(x, region_len)` to locate the peaks with a region length of 6 seconds. This function returns the highest peak in a neighborhood of $L_R$ samples. Set the result to  `est`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "This boundary detection algorithm has a number of parameters which must be tuned. One way of tuning parameters is to test a large variety of different parameters against a known ground truth set.\n",
    "\n",
    "In the following three exercises, you will create a system for optimizing some of the parameters of the structure features boundary detection algorithm using some of _The Beatles_ ground truth set.\n",
    "\n",
    "Create the function `song_boundary_score()`:\n",
    "\n",
    "Inputs:\n",
    "- `ssm`: $\\mathbf{S}$, the regular SSM of the song\n",
    "- `ff`: $F_f$, the sampling rate of $\\mathbf{S}$\n",
    "- `ref`: the reference annotations\n",
    "- `diag_len`: $L_D$, diagonal smooth length for creating $\\mathbf{S}_D$\n",
    "- `ssm_tau`: $\\tau$, the thresholding parameter for creating $\\mathbf{S}_{\\tau}$\n",
    "- `k_len`: $L_K$, derivative kernel length used for creating $\\mathbf{K}_{\\Delta}$\n",
    "- `region_len`: $L_R$, region length for finding peaks in $\\Delta_S[n]$\n",
    "\n",
    "Output: \n",
    "- the F-measure score of the estimated boundaries versus the reference annotations.\n",
    "\n",
    "Additional Notes:\n",
    "- Combine the elements of Exercises 4-6 to create the full algorithm in one function.\n",
    "- If you are worried that your solutions above might be off, you can use the `fmplib` versions of the functions.\n",
    "- Use $\\tau = 3$ seconds for the F-measure calculation (sorry, different parameter with the same Greek letter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def song_boundary_score(ssm, ff, ref, diag_len, ssm_tau, k_len, region_len):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this function with the files below, creating $\\mathbf{S}$ using `audio_to_ssm()` with CENS parameters $l = 25$ and $d = 3$.\n",
    "\n",
    "Use parameters: $L_D = 10$, $\\tau = 0.8$, $L_K = 80$, $L_R = 40$. You should get a score of about $0.47$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_file = \"beatles/beatles_oh_darling.wav\"\n",
    "ref_file = \"beatles/beatles_oh_darling.lab\"\n",
    "\n",
    "ex7_score = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "It is important to test on a large number of songs to get confidence in our parameter choices. First, gather the songs by writing the function `create_dataset()`. This represents the fixed data that does not change throughout the testing.\n",
    "\n",
    "Input:\n",
    "- `dir_path`: the directory path to search for audio and reference files.\n",
    "\n",
    "Output:\n",
    "- a list of tuples, one per song, where each tuple is `(ssm, ff, ref)` for that song using CENS parameters $l = 25$ and $d = 3$.\n",
    "\n",
    "The following python functions may be handy: `os.listdir`, `os.path.join`.\n",
    "\n",
    "Create the dataset for the directory `\"beatles\"`. This directory contains only 4 songs, but you can imagine working with a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def create_dataset(dir_path):\n",
    "    pass\n",
    "\n",
    "dataset = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write the function `average_boundary_score()` that computes the average F-measure for all songs in the dataset for a given set of parameters.\n",
    "\n",
    "Inputs:\n",
    "- `dataset`: the dataset of songs\n",
    "- `diag_len`: $L_D$, diagonal smooth length for creating $\\mathbf{S}_D$\n",
    "- `ssm_tau`: $\\tau$, the thresholding parameter for creating $\\mathbf{S}_{\\tau}$\n",
    "- `k_len`: $L_K$, derivative kernel length used for creating $\\mathbf{K}_{\\Delta}$\n",
    "- `region_len`: $L_R$, region length for finding peaks in $\\Delta_S[n]$\n",
    "\n",
    "Output:\n",
    "- the average F-measure boundary score across all the data in dataset using the given parameters.\n",
    "\n",
    "Use `song_boundary_score()` that you already wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_boundary_score(dataset, diag_len, ssm_tau, k_len, region_len):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the average boundary score using the Beatles dataset and the same parameters as in Exercise 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex8_score = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "\n",
    "The last step is to find a reasonably good parameter set that maximizes the average F-measure for the whole dataset. This problem is more open-ended. You should create the dataset once, and then repeatedly call `average_boundary_score` with different parameter sets, searching for the highest scoring set. \n",
    "\n",
    "It is useful to think about good operating bounds for these parameters. For example:\n",
    "- $L_D$: should probably be in the range $2-10$ seconds (but remember to convert to samples) \n",
    "- $\\tau$: $.5 - .95$\n",
    "- $L_K$: $10-50$ seconds\n",
    "- $L_R$: $10-30$ seconds \n",
    "\n",
    "\n",
    "You can use whatever search method you'd like. A good starting point is a grid-based search where you iterate through a finite set of parameter combinations, spaced over a 4-dimensional grid. You can try a rough initial search (with a course grid) to find a maxima, followed by a finer-grained search to hone in on the highest value in that neighborhood.\n",
    "\n",
    "Print the highest scoring parameters and the resulting F-measure. Keep all your code and experiments so we can see how you found your values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex9_score = ...\n",
    "ex9_params = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
